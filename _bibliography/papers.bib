---
---

@INPROCEEDINGS{ada_nns,
  author={Jung, Sungjun, Park, Yongsang, Lee, Haeun, Oh, Young. H, and Lee, Jae W.},
  booktitle={The ACM Web Conference 2025 (WWW)}, 
  title={Angular Distance-Guided Neighbor Selection for Graph-Based Approximate Nearest Neighbor Search (To appear)}, 
  year={2025},
  keywords={Approximate Nearest Neighbor Search; Similarity Search; Graph-based Approximate Nearest Neighbor Search},
  doi={},
  abbr={WWW},
  selected={true},
}

@INPROCEEDINGS{elsa_chip,
  author={Seo, Seong Hoon and Kim, Soosung and Jung, Sung Jun and Kwon, Sangwoo and Lee, Hyunseung and Lee, Jae W.},
  booktitle={ESSCIRC 2022- IEEE 48th European Solid State Circuits Conference (ESSCIRC)}, 
  title={A 40nm 5.6TOPS/W 239GOPS/mm2 Self-Attention Processor with Sign Random Projection-based Approximation}, 
  year={2022},
  volume={},
  number={},
  pages={85-88},
  keywords={Semiconductor device modeling;Computer vision;Computational modeling;Neural networks;Europe;Transformers;Approximation algorithms},
  doi={10.1109/ESSCIRC55480.2022.9911343},
  abbr={ESSCIRC},
  pdf={esscirc22_self-attention.pdf}
}

@INPROCEEDINGS{boss,
  author={Heo, Jun and Lee, Seung Yul and Min, Sunhong and Park, Yeonhong and Jung, Sung Jun and Jun Ham, Tae and Lee, Jae W.},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={BOSS: Bandwidth-Optimized Search Accelerator for Storage-Class Memory}, 
  year={2021},
  volume={},
  number={},
  pages={279-291},
  keywords={Energy consumption;Web services;Query processing;Random access memory;Bandwidth;Search engines;Throughput;full-text search;inverted index;near-data processing;storage class memory;hardware accelerator},
  doi={10.1109/ISCA52012.2021.00030},
  abbr={ISCA},
  pdf={isca21_boss}
}

@inproceedings{elsa,
  author = {Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W.},
  title = {ELSA: hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks},
  year = {2021},
  isbn = {9781450390866},
  publisher = {IEEE Press},
  url = {https://doi.org/10.1109/ISCA52012.2021.00060},
  doi = {10.1109/ISCA52012.2021.00060},
  abstract = {The self-attention mechanism is rapidly emerging as one of the most important key primitives in neural networks (NNs) for its ability to identify the relations within input entities. The self-attention-oriented NN models such as Google Transformer and its variants have established the state-of-the-art on a very wide range of natural language processing tasks, and many other self-attention-oriented models are achieving competitive results in computer vision and recommender systems as well. Unfortunately, despite its great benefits, the self-attention mechanism is an expensive operation whose cost increases quadratically with the number of input entities that it processes, and thus accounts for a significant portion of the inference runtime. Thus, this paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism. Specifically, based on the intuition that not all relations are equal, we devise a novel approximation scheme that significantly reduces the amount of computation by efficiently filtering out relations that are unlikely to affect the final output. With the specialized hardware for this approximate self-attention mechanism, ELSA achieves a geomean speedup of 58.1X as well as over three orders of magnitude improvements in energy efficiency compared to GPU on self-attention computation in modern NN models while maintaining less than 1\% loss in the accuracy metric.},
  booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
  pages = {692–705},
  numpages = {14},
  keywords = {neural network, hardware accelerator, attention},
  location = {Virtual Event, Spain},
  series = {ISCA '21},
  abbr={ISCA},
  pdf={isca21_elsa}
}

@inproceedings{cereal,
  author = {Jang, Jaeyoung and Jung, Sung Jun and Jeong, Sunmin and Heo, Jun and Shin, Hoon and Ham, Tae Jun and Lee, Jae W.},
  title = {A specialized architecture for object serialization with applications to big data analytics},
  year = {2020},
  isbn = {9781728146614},
  publisher = {IEEE Press},
  url = {https://doi.org/10.1109/ISCA45697.2020.00036},
  doi = {10.1109/ISCA45697.2020.00036},
  abstract = {Object serialization and deserialization (S/D) is an essential feature for efficient communication between distributed computing nodes with potentially non-uniform execution environments. S/D operations are widely used in big data analytics frameworks for remote procedure calls and massive data transfers like shuffles. However, frequent S/D operations incur significant performance and energy overheads as they must traverse and process a large object graph. Prior approaches improve S/D throughput by effectively hiding disk or network I/O latency with computation, increasing compression ratio, and/or application-specific customization. However, inherent dependencies in the existing (de)serialization formats and algorithms eventually become the major performance bottleneck. Thus, we propose Cereal, a specialized hardware accelerator for memory object serialization. By co-designing the serialization format with hardware architecture, Cereal effectively utilizes abundant parallelism in the S/D process to deliver high throughput. Cereal also employs an efficient object packing scheme to compress metadata such as object reference offsets and a space-efficient bitmap representation for the object layout. Our evaluation of Cereal using both a cycle-level simulator and synthesizable Chisel RTL demonstrates that Cereal delivers 43.4x higher average S/D throughput than 88 other S/D libraries on Java Serialization Benchmark Suite. For six Spark applications Cereal achieves 7.97x and 4.81x speedups on average for S/D operations over Java built-in serializer and Kryo, respectively, while saving S/D energy by 227.75x and 136.28x.},
  booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
  pages = {322–334},
  numpages = {13},
  keywords = {apache spark, data analytics, domain-specific architecture, hardware-software co-design, object serialization},
  location = {Virtual Event},
  series = {ISCA '20},
  abbr = {ISCA},
  pdf = {isca20_cereal.pdf}
}

@INPROCEEDINGS{a3,
  author={Ham, Tae Jun and Jung, Sung Jun and Kim, Seonghak and Oh, Young H. and Park, Yeonhong and Song, Yoonho and Park, Jung-Hun and Lee, Sanghee and Park, Kyoung and Lee, Jae W. and Jeong, Deog-Kyoon},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation}, 
  year={2020},
  volume={},
  number={},
  pages={328-341},
  keywords={Artificial neural networks;Hardware;Task analysis;Computational modeling;Bit error rate;Data models;attention mechanism;approximation;neural network;accelerators;Domain Specific Architectures},
  doi={10.1109/HPCA47549.2020.00035},
  abbr={HPCA},
  pdf={hpca20_a3.pdf}
}

@inproceedings{charon,
  author = {Jang, Jaeyoung and Heo, Jun and Lee, Yejin and Won, Jaeyeon and Kim, Seonghak and Jung, Sung Jun and Jang, Hakbeom and Ham, Tae Jun and Lee, Jae W.},
  title = {Charon: Specialized Near-Memory Processing Architecture for Clearing Dead Objects in Memory},
  year = {2019},
  isbn = {9781450369381},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3352460.3358297},
  doi = {10.1145/3352460.3358297},
  abstract = {Garbage collection (GC) is a standard feature for high productivity programming, saving a programmer from many nasty memory-related bugs. However, these productivity benefits come with a cost in terms of application throughput, worst-case latency, and energy consumption. Since the first introduction of GC by the Lisp programming language in the 1950s, a myriad of hardware and software techniques have been proposed to reduce this cost. While the idea of accelerating GC in hardware is appealing, its impact has been very limited due to narrow coverage, lack of flexibility, intrusive system changes, and significant hardware cost. Even with specialized hardware GC performance is eventually limited by memory bandwidth bottleneck. Fortunately, emerging 3D stacked DRAM technologies shed new light on this decades-old problem by enabling efficient near-memory processing with ample memory bandwidth. Thus, we propose Charon1, the first 3D stacked memory-based GC accelerator. Through a detailed performance analysis of HotSpot JVM, we derive a set of key algorithmic primitives based on their GC time coverage and implementation complexity in hardware. Then we devise a specialized processing unit to substantially improve their memory-level parallelism and throughput with a low hardware cost. Our evaluation of Charon with the full-production HotSpot JVM running two big data analytics frameworks, Spark and GraphChi, demonstrates a 3.29\texttimes{} geomean speedup and 60.7\% energy savings for GC over the baseline 8-core out-of-order processor.},
  booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages = {726–739},
  numpages = {14},
  keywords = {Domain-specific architecture, Garbage collection, Java Virtual Machine, Memory management, Near-memory processing},
  location = {Columbus, OH, USA},
  series = {MICRO '52},
  abbr={MICRO},
  pdf={micro19_charon.pdf}
}

@inproceedings{libnumber,
  author = {Oh, Young H. and Quan, Quan and Kim, Daeyeon and Kim, Seonghak and Heo, Jun and Jung, Sungjun and Jang, Jaeyoung and Lee, Jae W.},
  title = {A portable, automatic data qantizer for deep neural networks},
  year = {2018},
  isbn = {9781450359863},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3243176.3243180},
  doi = {10.1145/3243176.3243180},
  abstract = {With the proliferation of AI-based applications and services, there are strong demands for efficient processing of deep neural networks (DNNs). DNNs are known to be both compute-and memory-intensive as they require a tremendous amount of computation and large memory space. Quantization is a popular technique to boost efficiency of DNNs by representing a number with fewer bits, hence reducing both computational strength and memory footprint. However, it is a difficult task to find an optimal number representation for a DNN due to a combinatorial explosion in feasible number representations with varying bit widths, which is only exacerbated by layer-wise optimization. Besides, existing quantization techniques often target a specific DNN framework and/or hardware platform, lacking portability across various execution environments. To address this, we propose libnumber, a portable, automatic quantization framework for DNNs. By introducing Number abstract data type (ADT), libnumber encapsulates the internal representation of a number from the user. Then the auto-tuner of libnumber finds a compact representation (type, bit width, and bias) for the number that minimizes the user-supplied objective function, while satisfying the accuracy constraint. Thus, libnumber effectively separates the concern of developing an effective DNN model from low-level optimization of number representation. Our evaluation using eleven DNN models on two DNN frameworks targeting an FPGA platform demonstrates over 8\texttimes{} (7\texttimes{}) reduction in the parameter size on average when up to 7\% (1\%) loss of relative accuracy is tolerable, with a maximum reduction of 16\texttimes{}, compared to the baseline using 32-bit floating-point numbers. This leads to an geomean speedup of 3.79\texttimes{} with a maximum speedup of 12.77\texttimes{} over the baseline, while requiring only minimal programmer effort.},
  booktitle = {Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques},
  articleno = {17},
  numpages = {14},
  keywords = {quantization, performance, optimzation, deep neural networks, auto-tuning, approximate computing},
  location = {Limassol, Cyprus},
  series = {PACT '18},
  abbr = {PACT},
  pdf = {pact18_libnumber.pdf}
}
